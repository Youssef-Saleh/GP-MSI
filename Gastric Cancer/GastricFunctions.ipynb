{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage.color import lab2rgb\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from openTSNE import TSNE as opTSNE\n",
    "\n",
    "from sklearn.mixture import GaussianMixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "def CreateColorMap(NumberofColors , colorsArray ):\n",
    "    cmap = LinearSegmentedColormap.from_list('cmap', colorsArray, N=NumberofColors)\n",
    "    return cmap\n",
    "\n",
    "def CreateColorMap_Continuous(NumberofColors , colorsArray ):\n",
    "    cvals = np.arange(0,NumberofColors-1)\n",
    "    colors = colorsArray\n",
    "    norm=plt.Normalize(min(cvals),max(cvals))\n",
    "    tuples = list(zip(map(norm,cvals), colors))\n",
    "    cmap = LinearSegmentedColormap.from_list(\"\", tuples)\n",
    "    \n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadGastricData():\n",
    "    h5 = h5py.File('gastricData.mat', 'r')\n",
    "\n",
    "\n",
    "    HE_image= h5.get('HE_image')[:]\n",
    "    HE_image = HE_image.swapaxes(0,2)\n",
    "\n",
    "\n",
    "    MSI_data_cube = h5.get('MSI_data_cube')[:]\n",
    "    MSI_data_cube = MSI_data_cube.swapaxes(0,2)\n",
    "\n",
    "    goodlist= h5.get('goodlist')[:]\n",
    "    goodlist = goodlist.swapaxes(0,1)\n",
    "\n",
    "    peak_list= h5.get('peak_list')[:]\n",
    "    peak_list = peak_list.swapaxes(0,1)\n",
    "\n",
    "    pixel_to_sample_ID = h5.get('pixel_to_sample_ID')[:]\n",
    "    pixel_to_sample_ID = pixel_to_sample_ID.swapaxes(0,1)  \n",
    "    z = h5.get('z')[:]\n",
    "    h5.close()\n",
    "\n",
    "    # Clinical_data = pd.read_excel('ClinicalData.xlsx')\n",
    "\n",
    "    flattened_MSI_data_cube = MSI_data_cube.flatten().reshape(MSI_data_cube.shape[0] * MSI_data_cube.shape[1] , MSI_data_cube.shape[2])\n",
    "\n",
    "    flattened_pixel_to_sample_ID = pixel_to_sample_ID.flatten() \n",
    "    indices_of_background = np.where(flattened_pixel_to_sample_ID == -1)\n",
    "    sample_only_data = np.delete(flattened_MSI_data_cube, indices_of_background[0] , axis=0)\n",
    "    sample_ID_pixels = np.delete(flattened_pixel_to_sample_ID , indices_of_background[0] , axis=0)\n",
    "    # The scaler object (model)\n",
    "    scaler = StandardScaler()\n",
    "    # fit and transform the data\n",
    "    sample_only_scaled_data = scaler.fit_transform(sample_only_data) \n",
    "\n",
    "    return HE_image , MSI_data_cube , goodlist, peak_list, pixel_to_sample_ID, sample_only_data, sample_only_scaled_data , sample_ID_pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(ID ,sample_only_data, sample_only_scaled_data, sample_ID_pixels):\n",
    "\n",
    "    test_patient_indicies=np.where(sample_ID_pixels == ID)\n",
    "\n",
    "    mask = np.ones(sample_only_scaled_data.shape[0], dtype=bool)\n",
    "    mask[test_patient_indicies] = False\n",
    "\n",
    "    train_scaled_data = sample_only_scaled_data[mask]\n",
    "    train_data = sample_only_data[mask]\n",
    "    test_scaled_data = sample_only_scaled_data[test_patient_indicies]\n",
    "    test_data = sample_only_data[test_patient_indicies]\n",
    "\n",
    "\n",
    "    train_ID_pixels = sample_ID_pixels[sample_ID_pixels != ID]\n",
    "    test_ID_pixels = sample_ID_pixels[sample_ID_pixels == ID]\n",
    "\n",
    "    return train_scaled_data,train_data,train_ID_pixels,test_scaled_data,test_data,test_ID_pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tSNE(sample_only_scaled_data):\n",
    "    time_start = time.time()\n",
    "    tsne_results_op = TSNE(n_components=3,\n",
    "            perplexity = 50,\n",
    "            learning_rate = 200,\n",
    "            init = 'random',\n",
    "            random_state = 0,\n",
    "            early_exaggeration = 12,\n",
    "            n_iter = 1000,\n",
    "            verbose=True,\n",
    "            ).fit_transform(sample_only_scaled_data)\n",
    "\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    \n",
    "    return tsne_results_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_results(tsne_results):\n",
    "\n",
    "    kmeans_3 = KMeans(n_clusters=3, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "    kmeans_4 = KMeans(n_clusters=4, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "    kmeans_5 = KMeans(n_clusters=5, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "    kmeans_6 = KMeans(n_clusters=6, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "    kmeans_7 = KMeans(n_clusters=7, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "    kmeans_8 = KMeans(n_clusters=8, n_init=100, max_iter=2000, random_state=0, verbose=0 ).fit(tsne_results)\n",
    "\n",
    "    return kmeans_3 , kmeans_4, kmeans_5, kmeans_6, kmeans_7, kmeans_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadClinicalData(ID, file):\n",
    "    Clinical_data = pd.read_excel(file)\n",
    "    Clinical_data = Clinical_data.drop(labels=ID-1,axis=0)\n",
    "    Clinical_data = Clinical_data.reset_index(drop=True)\n",
    "\n",
    "    return Clinical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KaplanMeierSurvivalFitter( labels , colors ,Clinical_data , sample_ID_pixels , SignCluster=[]):\n",
    "\n",
    "    Clinical_data_copied = Clinical_data.copy(deep=True)\n",
    "    \n",
    "\n",
    "    labels_count = len(np.unique(labels))\n",
    "    Clusters = [ [] for _ in range(labels_count) ]\n",
    "\n",
    "    for i in range(1,len(Clinical_data)+1):\n",
    "        Pixels_Samples = np.where(sample_ID_pixels == i)[0]\n",
    "        Patient_Labels = labels[Pixels_Samples]\n",
    "\n",
    "        for cluster_label in range(labels_count):\n",
    "            Patient_Pixels = Patient_Labels[Patient_Labels == cluster_label]\n",
    "            if len(Patient_Pixels) >= int( (1/labels_count * len(Patient_Labels))):\n",
    "                Clusters[cluster_label].append(i)\n",
    "    \n",
    "    Belong_Clusters = [ [] for _ in range(labels_count) ]\n",
    "\n",
    "    for i in range(1, len(Clinical_data)+1):\n",
    "        for j in range(labels_count):\n",
    "            if (i in Clusters[j]):\n",
    "                Belong_Clusters[j].append(1)\n",
    "            else:\n",
    "                Belong_Clusters[j].append(0)\n",
    "\n",
    "    for cluster_label in range(labels_count):\n",
    "        Clinical_data_copied[\"Belong_Cluster_\" + str(cluster_label+1)] = Belong_Clusters[cluster_label]\n",
    "    \n",
    "\n",
    "    kaplan_fitters = [ [] for _ in range(labels_count) ]\n",
    "    axes = [ [] for _ in range(labels_count)]\n",
    "    Clusters = [ [] for _ in range(labels_count) ]\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    if not SignCluster:\n",
    "\n",
    "        for cluster_label in range(labels_count):\n",
    "            kaplan_fitters[cluster_label] = KaplanMeierFitter() ## instantiate the class to create an object\n",
    "            Clusters[cluster_label] = Clinical_data_copied.query(\"Belong_Cluster_\" + str(cluster_label+1) + \" == 1\")\n",
    "            kaplan_fitters[cluster_label].fit(Clusters[cluster_label][\"Surv_time\"], Clusters[cluster_label][\"Surv_status\"], label='Cluster ' + str(cluster_label+1))\n",
    "            axes[cluster_label] = kaplan_fitters[cluster_label].plot(ci_show=False)\n",
    "\n",
    "\n",
    "        for cluster_label in range(labels_count):\n",
    "            legend = axes[cluster_label].get_legend()\n",
    "            hl_dict = {handle.get_label(): handle for handle in legend.legendHandles}\n",
    "            hl_dict['Cluster ' + str(cluster_label+1)].set_color(colors[cluster_label])\n",
    "            axes[cluster_label].get_lines()[cluster_label].set_color(colors[cluster_label])\n",
    "    else:\n",
    "\n",
    "        for cluster_label in SignCluster:\n",
    "            kaplan_fitters[cluster_label] = KaplanMeierFitter() ## instantiate the class to create an object\n",
    "            Clusters[cluster_label] = Clinical_data_copied.query(\"Belong_Cluster_\" + str(cluster_label+1) + \" == 1\")\n",
    "            kaplan_fitters[cluster_label].fit(Clusters[cluster_label][\"Surv_time\"], Clusters[cluster_label][\"Surv_status\"], label='Cluster ' + str(cluster_label+1))\n",
    "            axes[cluster_label] = kaplan_fitters[cluster_label].plot(ci_show=False)\n",
    "\n",
    "        incr=0\n",
    "\n",
    "        for cluster_label in SignCluster:\n",
    "            legend = axes[cluster_label].get_legend()\n",
    "            hl_dict = {handle.get_label(): handle for handle in legend.legendHandles}\n",
    "            hl_dict['Cluster ' + str(cluster_label+1)].set_color(colors[cluster_label])\n",
    "            axes[cluster_label].get_lines()[incr].set_color(colors[cluster_label])\n",
    "            incr+=1\n",
    "            \n",
    "    plt.title(\"Kaplan Meier Graph\")\n",
    "    plt.xlabel('Survival time (month)')\n",
    "    plt.ylabel('Probability of Survival')\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0,60])\n",
    "    plt.tight_layout()\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return Clinical_data_copied\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogRankTest_PrintValues(labels,Clinical_data, printResults = False):\n",
    "\n",
    "   labels_count = len(np.unique(labels))\n",
    "   Results = [ [ [] for _ in range(labels_count) ] for _ in range(labels_count) ]\n",
    "   Clusters = [ [] for _ in range(labels_count) ]\n",
    "\n",
    "   for cluster_label in range(labels_count):\n",
    "      Clusters[cluster_label] = Clinical_data.query(\"Belong_Cluster_\" + str(cluster_label+1) + \" == 1\")\n",
    "\n",
    "   for cluster_label_main in range(labels_count):\n",
    "\n",
    "      for cluster_label_secondary in range(labels_count):\n",
    "\n",
    "         if cluster_label_main == cluster_label_secondary:\n",
    "               Results[cluster_label_main][cluster_label_secondary] = None\n",
    "         else:\n",
    "               Results[cluster_label_main][cluster_label_secondary] = logrank_test(\n",
    "               Clusters[cluster_label_main][\"Surv_time\"],\n",
    "               Clusters[cluster_label_secondary][\"Surv_time\"],\n",
    "               Clusters[cluster_label_main][\"Surv_status\"] , \n",
    "               Clusters[cluster_label_secondary][\"Surv_status\"])\n",
    "\n",
    "               if printResults == True:\n",
    "                  print(\"Cluster \" + str(cluster_label_main+1) + \" with Cluster \" + str(cluster_label_secondary+1) )\n",
    "                  Results[cluster_label_main][cluster_label_secondary].print_summary()\n",
    "                  print(\"\\n\")\n",
    "   return Results\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CoxHazardFitter(labels , Clinical_data):\n",
    "\n",
    "    Clinical_data_coxHazard = Clinical_data.copy(deep=True)\n",
    "    Clinical_data_coxHazard.drop([\"Sample_ID\",\"T\",\"N\",\"M\"],inplace = True,axis=1)\n",
    "\n",
    "    labels_count = len(np.unique(labels))\n",
    "    Clusters = [ [] for _ in range(labels_count) ]\n",
    "\n",
    "    for cluster_label in range(labels_count):\n",
    "        Clusters[cluster_label] = Clinical_data_coxHazard.query(\"Belong_Cluster_\" + str(cluster_label+1) + \" == 1\")\n",
    "\n",
    "    for cluster_label in range(labels_count):\n",
    "        Clinical_data_coxHazard.drop([\"Belong_Cluster_\" + str(cluster_label+1)],inplace = True,axis=1)\n",
    "        Clinical_data_coxHazard[\"Dead in Cluster \" + str(cluster_label+1)] = Clusters[cluster_label][\"Surv_status\"]\n",
    "\n",
    "\n",
    "    Clinical_data_coxHazard = Clinical_data_coxHazard.fillna(0)\n",
    "\n",
    "    # Applying CoxHazard\n",
    "    cph=CoxPHFitter(penalizer=0.001)\n",
    "\n",
    "    cph.fit(Clinical_data_coxHazard, \"Surv_time\", \"Surv_status\")\n",
    "\n",
    "    cph.plot(hazard_ratios=True)\n",
    "    cph.print_summary()\n",
    "\n",
    "    return cph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CoxHazardBarPlot( cph , clusters, colors, max_tick_value):\n",
    "\n",
    "    hazard_ratio = cph.summary[\"exp(coef)\"]\n",
    "    hazard_ratio = hazard_ratio.to_numpy()\n",
    "    \n",
    "    cmap = CreateColorMap_Continuous(len(colors)+1,colors)\n",
    "\n",
    "    rescale = lambda hazard_ratio: ((hazard_ratio - np.min(hazard_ratio)) / (np.max(hazard_ratio) - np.min(hazard_ratio)))\n",
    "    labels_count = len(clusters)\n",
    "    x = np.arange(1,labels_count + 1)\n",
    "    y = [ ]\n",
    "    for cluster_label in range(labels_count):\n",
    "        y.append(len(clusters[cluster_label]))\n",
    "\n",
    "    bar = plt.bar(x, y,color = cmap(rescale(hazard_ratio)))\n",
    "    plt.xticks(np.arange(1,labels_count+1))\n",
    "    plt.title('Number of patients per subpopulation')\n",
    "    sm = ScalarMappable(cmap=cmap, norm=plt.Normalize(0,np.max(hazard_ratio) ) )\n",
    "\n",
    "    cbar = plt.colorbar(sm, aspect=10,shrink=0.9, pad=0.03)\n",
    "    \n",
    "    max_tick = max_tick_value # variable to be edited for max tick\n",
    "    cbar.ax.tick_params(size=0)\n",
    "    cbar.set_ticks([0,max_tick]) # Comment the colorbar lines and look at the tick values to find the max value tick to be edited\n",
    "    cbar.ax.set_yticklabels(['Low','High'],weight='bold',fontsize=20) \n",
    "    cbar.set_label(\"Hazard\",labelpad= -2)\n",
    " \n",
    "    # plt.yticks([])\n",
    "    plt.show()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAM_Analysis(labels, Clinical_data, sample_ID_pixels, hazardous_cluster_label,sample_only_data,peak_list):\n",
    "\n",
    "    labels_count=len(np.unique(labels))\n",
    "    indices_of_patients = [[] for _ in range(len(Clinical_data))]\n",
    "    Sig_Cluster=[[] for _ in range(len(Clinical_data))]\n",
    "    Status=[[] for _ in range(len(Clinical_data))]\n",
    "\n",
    "    index = 0\n",
    "    for i in Clinical_data[\"Sample_ID\"].tolist():\n",
    "        Pixels_Samples = np.where(sample_ID_pixels == i)[0]\n",
    "        Patient_Labels = labels[Pixels_Samples]\n",
    "        \n",
    "        for cluster_label in range(labels_count):\n",
    "\n",
    "            Patient_Pixels = Patient_Labels[Patient_Labels == cluster_label]\n",
    "\n",
    "            if len(Patient_Pixels) >= int((1/labels_count * len(Patient_Labels))):\n",
    "\n",
    "                if cluster_label != hazardous_cluster_label:\n",
    "                    \n",
    "                    Status[index].append(1)\n",
    "                else:\n",
    "                    Status[index].append(2)  \n",
    "\n",
    "\n",
    "                Sig_Cluster[index].append(cluster_label)\n",
    "\n",
    "        for j in range(0,len(Sig_Cluster[index])):\n",
    "            \n",
    "            indices=np.where(labels == Sig_Cluster[index][j])[0]\n",
    "\n",
    "            # for element in indices:\n",
    "            #     if element in Pixels_Samples:\n",
    "                    \n",
    "            #         indices_of_patients[i-1].append(element)\n",
    "\n",
    "            indices_of_patients[index].extend(list(set(Pixels_Samples).intersection(indices)))\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    Final_Status=[]\n",
    "    for patient in Status:\n",
    "        if 2 in patient:\n",
    "            Final_Status.append(2)\n",
    "        else:\n",
    "            Final_Status.append(1)\n",
    "    \n",
    "    Unique_IDs = Clinical_data[\"Sample_ID\"].tolist()\n",
    "    Metastasis_Patients = [ ]\n",
    "    Average_protein_values = [ ]\n",
    "\n",
    "    for i in range(0,len(Unique_IDs)):\n",
    "        Patient_MSI_values = sample_only_data[indices_of_patients[i]]\n",
    "        Metastasis_Patients.append(Patient_MSI_values)\n",
    "        Average_protein_values.append(np.average(Metastasis_Patients[i], axis=0))\n",
    "    \n",
    "    protein_dataframe = pd.DataFrame(Average_protein_values,columns=peak_list[:,0].astype(int))\n",
    "    protein_dataframe[\"Status\"] = Final_Status\n",
    "\n",
    "    return protein_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SignificantClusters(labels, Results):\n",
    "\n",
    "    labels_count = len(np.unique(labels))\n",
    "    pvalue_list =   [ [] for _ in range(labels_count) ]\n",
    "    min_pvalues = [ ]\n",
    "\n",
    "    for i in range(labels_count):\n",
    "        for j in range(labels_count):\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                pvalue_list[i].append(round(Results[i][j].p_value,2))\n",
    "\n",
    "    for i in range(labels_count):\n",
    "        min_pvalues.append(np.min(pvalue_list[i]))\n",
    "\n",
    "    SignCluster = [ ]\n",
    "    for i in range(len(min_pvalues)):\n",
    "        if min_pvalues[i] == np.min(min_pvalues):\n",
    "            SignCluster.append(i)\n",
    "\n",
    "    pvalue = np.min(min_pvalues)\n",
    "    return pvalue , SignCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSignificantProteins(file , delete=False):\n",
    "    import json\n",
    "    with open(file) as f:\n",
    "        proteins = json.load(f)\n",
    "\n",
    "\n",
    "    edited_proteins = [ ]\n",
    "    for protein in proteins:\n",
    "        for string in protein:\n",
    "            string = string[1::]\n",
    "            string = int(string)\n",
    "            edited_proteins.append(string)\n",
    "\n",
    "    # To delete the file, in order to be not confused with old versions of same file\n",
    "    if delete == True:\n",
    "        import os\n",
    "        os.remove(file)\n",
    "\n",
    "    return edited_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as labels for the SVM/KNN models, turns all cluster labels in kmeans labels into 1 or 2 (based on breast data)\n",
    "def TargetLabelsCreation(labels , Clinical_data, sample_ID_pixels, hazardous_cluster_label , survival_cluster_label):\n",
    "\n",
    "    labels_count = len(np.unique(labels))\n",
    "    Clusters = [ [] for _ in range(labels_count) ]\n",
    "    Target_labels=copy.deepcopy(labels)\n",
    "\n",
    "    # Change to the correct clusters identified in the survival analysis (use index not actual value)\n",
    "    Poor_survival_cluster = hazardous_cluster_label\n",
    "    High_survival_cluster = survival_cluster_label\n",
    "\n",
    "    Poor = 1\n",
    "    Moderate = 2\n",
    "    High = 3\n",
    "\n",
    "    for i in range(1,len(Clinical_data)+1):\n",
    "        Pixels_Samples = np.where(sample_ID_pixels == i)[0]\n",
    "        Patient_Labels = labels[Pixels_Samples]\n",
    "\n",
    "        for cluster_label in range(labels_count):\n",
    "            Patient_Pixels = Patient_Labels[Patient_Labels == cluster_label]\n",
    "            if len(Patient_Pixels) >= int( (1/labels_count * len(Patient_Labels))):\n",
    "                Clusters[cluster_label].append(i)\n",
    "\n",
    "    for i in Clusters[Poor_survival_cluster]:\n",
    "        Pixels_Samples = np.where(sample_ID_pixels == i)[0]\n",
    "        Target_labels[Pixels_Samples] = Poor\n",
    "\n",
    "    Target_labels[Target_labels != Poor] = Moderate\n",
    "\n",
    "    for i in Clusters[High_survival_cluster]:\n",
    "        Pixels_Samples = np.where(sample_ID_pixels == i)[0]\n",
    "        Target_labels[Pixels_Samples] = High\n",
    "\n",
    "    return Target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(used_kernel, regularization_value ,train_data,train_labels,test_data):\n",
    "    from sklearn import svm\n",
    "\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel=used_kernel , C = regularization_value)\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(train_data, train_labels)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(test_data)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(neighbours, train_data, train_labels, test_data):\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbours)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    model.fit(train_data,train_labels)\n",
    "\n",
    "    predicted_labels= model.predict(test_data)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProbabilityCalc(y_pred,Poor=1,Moderate=2,High=3):\n",
    "    \n",
    "    Probability_arr=np.unique(y_pred,return_counts=True)\n",
    "    \n",
    "    Total_Propability = 0\n",
    "    Poor_Surv = 0\n",
    "    Moderate_Surv = 0\n",
    "    High_Surv = 0\n",
    "\n",
    "    for Probability in Probability_arr[1]:\n",
    "        Total_Propability += Probability\n",
    "\n",
    "    increment = 0\n",
    "    for Surv_label in Probability_arr[0]:\n",
    "        if Surv_label == Poor:\n",
    "            Poor_Surv = (Probability_arr[1][increment] / Total_Propability) * 100\n",
    "        elif Surv_label == Moderate:\n",
    "            Moderate_Surv = (Probability_arr[1][increment] / Total_Propability) * 100\n",
    "        elif Surv_label == High:\n",
    "            High_Surv = (Probability_arr[1][increment] / Total_Propability) * 100\n",
    "        increment += 1\n",
    "\n",
    "    print(\"Poor survival probability : {} \\nModerate survival probability : {} \\nHigh survival probability : {}\".format(Poor_Surv,Moderate_Surv,High_Surv))\n",
    "\n",
    "    return Poor_Surv , Moderate_Surv , High_Surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputDataframe(total_results_dataframe,patient_ID , Clinical_data, Poor_Surv, Moderate_Surv, High_Surv, no_of_clusters, SAM_protein):\n",
    "    \n",
    "    results_dataframe = {}\n",
    "    results_dataframe[\"Patient to be predicted/left out\"] = patient_ID\n",
    "    results_dataframe[\"Poor Survival Subpopulation\"] = Poor_Surv\n",
    "    results_dataframe[\"Moderate Survival Subpopulation\"] = Moderate_Surv\n",
    "    results_dataframe[\"Good Survival Subpopulation\"] = High_Surv\n",
    "    results_dataframe[\"Surv(months)\"] = Clinical_data[\"Surv_time\"][patient_ID-1]\n",
    "    results_dataframe[\"Survival Status\"] = Clinical_data[\"Surv_status\"][patient_ID-1]\n",
    "    prediction = \"Poor\"\n",
    "\n",
    "    if Poor_Surv <= 10:\n",
    "        prediction = \"High\"\n",
    "    elif Poor_Surv > 10 and Poor_Surv <= 50:\n",
    "        prediction = \"Moderate\"\n",
    "    else:\n",
    "        prediction = \"Poor\"\n",
    "        \n",
    "    results_dataframe[\"Predicted Survivability\"] = prediction\n",
    "    results_dataframe[\"Number of Clusters\"] = no_of_clusters\n",
    "    results_dataframe[\"SAM Features for each tSNE run on new subset\"] = \"Significant Features : m/z = \" + str(SAM_protein)\n",
    "\n",
    "    results_dataframe = pd.DataFrame([results_dataframe])\n",
    "    total_results_dataframe = total_results_dataframe.append(results_dataframe)\n",
    "    return total_results_dataframe"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a862fa755b339cb269ec5ceb6e73837f63b6953002ae94843c7ff6941dd7636"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
